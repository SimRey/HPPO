{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "from gym.utils import seeding\n",
    "\n",
    "import random\n",
    "\n",
    "from a import PPO\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Heater(Env):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.max_time = 30\n",
    "\n",
    "        # Action declaration\n",
    "        self.action_space = Dict({\n",
    "            \"discrete\": Discrete(3), # Discrete actions up, down, stay\n",
    "            \"continuous\": Box(low=np.array([0., ]), high=np.array([1., ]), dtype=np.float32) \n",
    "        })\n",
    "\n",
    "        # Temperature array\n",
    "        self.low = np.array([0.,])\n",
    "        self.high = np.array([100.,])\n",
    "        self.observation_space = Box(low=self.low, high=self.high, dtype=np.float32)\n",
    "        \n",
    "        # Set start temp and start time\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def seed(self,seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "\n",
    "    def step(self, action):\n",
    "        temp = self.state[0]\n",
    "\n",
    "        d_action = action[\"discrete\"]\n",
    "\n",
    "        c_action = action[\"continuous\"]\n",
    "        c_action = self.interpolation(c_action)[0]\n",
    "        \n",
    "        if d_action == 0: # Increase temperature\n",
    "            temp += c_action\n",
    "        \n",
    "        elif d_action == 1: # Decrease temperature\n",
    "            temp -= c_action\n",
    "        \n",
    "        elif d_action == 2:\n",
    "            temp += 0\n",
    "        \n",
    "        # Reward function\n",
    "\n",
    "        if self.time > 0:\n",
    "            self.reward -= abs(38.0 - temp) \n",
    "            \n",
    "            if 37.8 <= temp <= 38.2:\n",
    "                reward = 100\n",
    "                self.done = True\n",
    "            \n",
    "            if abs(38.0 - temp) >= 20:\n",
    "                reward = -100\n",
    "                self.done = True\n",
    "\n",
    "        # Reduce time by 1 second\n",
    "        self.time -= 1 \n",
    "\n",
    "\n",
    "        self.state = np.array([temp, ], dtype=np.float32)\n",
    "\n",
    "        if self.time <= 0:\n",
    "            self.done = True\n",
    "            \n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, self.reward, self.done, info\n",
    "    \n",
    "    def interpolation(self, x):\n",
    "        # interpolation\n",
    "        y1 = 0.\n",
    "        y2 = 5.\n",
    "        x1 = 0.\n",
    "        x2 = 1.\n",
    "\n",
    "        y = y1 + ((y2 - y1)/(x2 - x1))*(x-x1)\n",
    "        return y\n",
    "\n",
    "    def render(self):\n",
    "        # Implement visualization --> in this case is not built\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset shower temperature\n",
    "        temp =  38. + random.randint(-5, 5)\n",
    "        self.state = np.array([temp, ], dtype=np.float32)\n",
    "\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        \n",
    "        # Reset shower time\n",
    "        self.time = self.max_time \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Heater()\n",
    "episodes = 10\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    temps = []\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  \n",
    "        temp, reward, done, info = env.step(action)\n",
    "        temps.append(temp) \n",
    "        score +=reward\n",
    "    mean_temp = np.mean(np.array(temps))\n",
    "    print(f'Episode: {episode}, Mean temperature: {mean_temp:.2f} Score: {score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, model, render, steps_per_epoch):\n",
    "    scores = 0\n",
    "    turns = 3\n",
    "    for j in range(turns):\n",
    "        s, done, ep_r, steps = env.reset(), False, 0, 0\n",
    "        while not (done or (steps >= steps_per_epoch)):\n",
    "            # Take deterministic actions at test time\n",
    "            action_d, action_c = model.evaluate(s)\n",
    "            action = {\n",
    "                \"discrete\": action_d,\n",
    "                \"continuous\": action_c \n",
    "            }\n",
    "            s_prime, r, done, info = env.step(action)\n",
    "\n",
    "            ep_r += r\n",
    "            steps += 1\n",
    "            s = s_prime\n",
    "            if render:\n",
    "                env.render()\n",
    "        scores += ep_r\n",
    "    return scores/turns\n",
    "\n",
    "\n",
    "def plot_learning_curve(x, scores):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    \n",
    "    plt.plot(x, running_avg, label=\"Running average\")\n",
    "    plt.plot(x, scores, alpha=0.4)\n",
    "    plt.title('Learning plot')\n",
    "    plt.xlabel(\"Runs\")\n",
    "    plt.ylabel(\"Scores\")\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import random\n",
    "\n",
    "    random_seed = 0\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    env = Heater()\n",
    "    env.seed(random_seed)\n",
    "\n",
    "    # Evaluation environment\n",
    "    eval_env = Heater()\n",
    "    eval_env.seed(random_seed)\n",
    "\n",
    "    # Hyperparameters\n",
    "    kwargs = {\n",
    "        \"state_dim\": env.observation_space.shape[0], \n",
    "        \"actions\": env.action_space, \n",
    "        \"env_with_Dead\": True,\n",
    "        \"gamma\": 0.99, \n",
    "        \"gae_lambda\": 0.95, \n",
    "        \"policy_clip\": 0.2, \n",
    "        \"n_epochs\": 10, \n",
    "        \"net_width\": 128, \n",
    "        \"lr\": 3e-4, \n",
    "        \"l2_reg\": 1e-3, \n",
    "        \"batch_size\": 256,\n",
    "        \"adv_normalization\": True, \n",
    "        \"entropy_coef\": 0.01,        # ðŸ”§ reduced for stability\n",
    "        \"entropy_coef_decay\": 0.9998\n",
    "    }\n",
    "\n",
    "    N = 2048                     # length of long trajectory\n",
    "    max_steps = env.max_time     # max steps per episode\n",
    "    Max_train_steps = int(1e3)\n",
    "    save_interval = int(10e3)\n",
    "    eval_interval = int(5e3)\n",
    "    best_interval = int(50e3)\n",
    "    best_score = -1e9\n",
    "\n",
    "    if not os.path.exists('model'): \n",
    "        os.mkdir('model')\n",
    "    if not os.path.exists('best_model'): \n",
    "        os.mkdir('best_model')\n",
    "    \n",
    "    model = PPO(**kwargs)\n",
    "\n",
    "    traj_length = 0\n",
    "    total_steps = 0\n",
    "    score_history = []\n",
    "    update = 0\n",
    "    num_updates = Max_train_steps // N\n",
    "\n",
    "    while total_steps < Max_train_steps:\n",
    "        obs = env.reset()\n",
    "        done, steps, score = False, 0, 0\n",
    "\n",
    "        # ---------- Interact with environment ----------\n",
    "        while not done and steps < max_steps:\n",
    "            steps += 1\n",
    "            traj_length += 1\n",
    "            total_steps += 1\n",
    "\n",
    "            action_d, probs_d, action_c, probs_c = model.select_action(obs)\n",
    "            action = {\"discrete\": action_d, \"continuous\": action_c}\n",
    "            obs_, reward, done, info = env.step(action)\n",
    "\n",
    "            dw = bool(done and steps != max_steps)  # dead/win detection\n",
    "            model.put_data((obs, action_d, action_c, reward, obs_, probs_d, probs_c, done, dw))\n",
    "\n",
    "            obs = obs_\n",
    "            score += reward\n",
    "\n",
    "            # ---------- Train ----------\n",
    "            if traj_length % N == 0:\n",
    "                a_losses, c_loss, entropies = model.train()\n",
    "                traj_length = 0\n",
    "                update += 1\n",
    "\n",
    "                # Linear LR decay\n",
    "                frac = 1.0 - (update - 1.0) / num_updates\n",
    "                lrnow = frac * kwargs[\"lr\"]\n",
    "                model.optimizer_d.param_groups[0][\"lr\"] = lrnow\n",
    "                model.optimizer_c.param_groups[0][\"lr\"] = lrnow\n",
    "                model.critic_optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "            # ---------- Evaluate ----------\n",
    "            if total_steps % eval_interval == 0:\n",
    "                eval_score = evaluate_policy(eval_env, model, False, max_steps)\n",
    "                score_history.append(eval_score)\n",
    "                print(\n",
    "                    f\"Env: Heater | Steps: {int(total_steps/1000)}k \"\n",
    "                    f\"| Eval score: {eval_score:.2f}\"\n",
    "                )\n",
    "\n",
    "            # ---------- Save ----------\n",
    "            if total_steps % save_interval == 0:\n",
    "                model.save(total_steps)\n",
    "\n",
    "            # ---------- Best model ----------\n",
    "            if total_steps >= best_interval:\n",
    "                if score_history and score_history[-1] > best_score:\n",
    "                    best_score = score_history[-1]\n",
    "                    model.best_save()\n",
    "\n",
    "        print(f\"Episode done | Steps: {steps} | Score: {score:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # ---------- Plot learning curve ----------\n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plot_learning_curve(x, score_history)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "env = Heater()\n",
    "# Hyperparameters\n",
    "kwargs = {\n",
    "    \"state_dim\": env.observation_space.shape[0], \n",
    "    \"actions\": env.action_space, \n",
    "    \"env_with_Dead\": True,\n",
    "    \"gamma\": 0.99, \n",
    "    \"gae_lambda\": 0.95, \n",
    "    \"policy_clip\": 0.2, \n",
    "    \"n_epochs\": 10, \n",
    "    \"net_width\": 128, \n",
    "    \"lr\": 3e-4, \n",
    "    \"l2_reg\": 1e-3, \n",
    "    \"batch_size\": 1,\n",
    "    \"adv_normalization\": True, \n",
    "    \"entropy_coef\": 0, \n",
    "    \"entropy_coef_decay\": 0.9998\n",
    "}\n",
    "\n",
    "\n",
    "model = PPO(**kwargs)\n",
    "\n",
    "model.load_best()\n",
    "scores = []\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    obs = env.reset()\n",
    "    actions = []\n",
    "    score = 0\n",
    "    while True:\n",
    "        action_d, _, action_c,_ = model.select_action(obs)\n",
    "        action = {\n",
    "                    \"discrete\": action_d,\n",
    "                    \"continuous\": action_c \n",
    "                }\n",
    "        print(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        actions.append(action)\n",
    "\n",
    "        if done:\n",
    "            print(f\"Done, points: {score}\")\n",
    "            break\n",
    "    \n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Mean score: {np.mean(scores)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
